{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAT330 - 6. A neural network\n",
    "<span style=\"color:blue\"> **This notebook train a deep neural networks** </span>\n",
    "\n",
    "<span style=\"color:red\"> ** - Can you play with the network architecture to improve the prediction? ** </span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, Concatenate\n",
    "import keras\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specify the categorical features\n",
    "categorical_features = ['basin','nature']\n",
    "\n",
    "\n",
    "train_filename = 'https://raw.githubusercontent.com/brajard/MAT330-Practical-work/master/data/train.csv'\n",
    "\n",
    "data_train = pd.read_csv(train_filename, dtype={cat: 'category' for cat in categorical_features})\n",
    "\n",
    "# Extract the predictor (but not the target -> data leakage)\n",
    "X = data_train.drop('target',axis=1, inplace=False)\n",
    "y = data_train['target']\n",
    "\n",
    "# Do the same with the test dataset\n",
    "test_filename = 'https://raw.githubusercontent.com/brajard/MAT330-Practical-work/master/data/test.csv'\n",
    "\n",
    "Xtest = pd.read_csv(test_filename, dtype={cat: 'category' for cat in categorical_features})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep-learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_image(df,sparam):\n",
    "    grid_l=11 # size of all 2D-grids (in pixels)\n",
    "    N = df.shape[0] #Number of samples\n",
    "    images = np.zeros(shape=(N,grid_l,grid_l))\n",
    "    for i in range(grid_l):\n",
    "        for j in range(grid_l):\n",
    "            images[:,i,j] = df[sparam+'_'+str(i)+'_'+str(j)]\n",
    "    return images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Features 0D to use\n",
    "features = ['latitude', 'instant_t','longitude', 'windspeed',\n",
    "       'hemisphere', 'Jday_predictor', 'initial_max_wind',\n",
    "       'max_wind_change_12h','dist2land']\n",
    "\n",
    "\n",
    "Xin0D = X.get(features)\n",
    "Xin0D_test = Xtest.get(features)\n",
    "\n",
    "# Equalization of the types:\n",
    "Xin0D = Xin0D.astype(float)\n",
    "Xin0D_test = Xin0D_test.astype(float)\n",
    "\n",
    "# Dummy encoding of categorical features\n",
    "Xcat = pd.get_dummies(X[categorical_features],prefix=categorical_features)\n",
    "Xcat_test = pd.get_dummies(Xtest[categorical_features],prefix=categorical_features)\n",
    "\n",
    "#Image features\n",
    "Xz = extract_image(X,'z')\n",
    "Xu = extract_image(X,'u')\n",
    "Xv = extract_image(X,'v')\n",
    "Xz_test = extract_image(Xtest,'z')\n",
    "Xu_test = extract_image(Xtest,'u')\n",
    "Xv_test = extract_image(Xtest,'v')\n",
    "\n",
    "\n",
    "#Stack the 3 images togegher\n",
    "X2d = np.stack((Xz,Xu,Xv),axis=-1)\n",
    "X2d_test = np.stack((Xz_test,Xu_test,Xv_test),axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have a look an image\n",
    "sample_id = 20 # sample number plotted - you can change it to see other storms and other instants\n",
    "params = ['z','u','v']\n",
    "fig, ax = plt.subplots(ncols = 3)\n",
    "ax[0].imshow(Xz[20],interpolation='nearest', origin='lower', cmap='seismic')\n",
    "ax[0].set_xlabel('param z')\n",
    "ax[1].imshow(Xu[20],interpolation='nearest', origin='lower', cmap='seismic')\n",
    "ax[1].set_xlabel('param u')\n",
    "ax[2].imshow(Xv[20],interpolation='nearest', origin='lower', cmap='seismic')\n",
    "ax[2].set_xlabel('param v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() #To reset the keras state\n",
    "\n",
    "#Input for the 0d layer:\n",
    "size_0d = Xin0D.shape[1]\n",
    "input_0d = Input(shape=(size_0d,),name='input_0d')\n",
    "###########################\n",
    "\n",
    "\n",
    "#Input for the categorical data:\n",
    "size_cat = Xcat.shape[1]\n",
    "input_cat = Input(shape=(size_cat,))\n",
    "embedding = Dense(2,activation = 'relu', name='embedding')(input_cat)\n",
    "############################\n",
    "\n",
    "#Input for image data:\n",
    "image_width = 11\n",
    "nchannel = X2d.shape[-1] #Number of image parameters\n",
    "nfilter1 = 10 #Number of filters of the firsgt layer\n",
    "nfilter2 = 1 #Number of filters of the second layer\n",
    "\n",
    "input_im = Input(shape=(image_width, image_width, nchannel),name='input_im2d')\n",
    "#First convolutional lauer\n",
    "layer_im1 = Conv2D(filters = nfilter1, kernel_size = 5, activation = 'relu')(input_im) #output size: nfilter1x7x7\n",
    "layer_im2 = Conv2D(filters = nfilter2, kernel_size = 5, activation = 'relu')(layer_im1) #output size: nfilter2x4x4\n",
    "flayer = Flatten(name='input_im0d')(layer_im2) #reshape to a 1-d layer: 16x1\n",
    "############################\n",
    "\n",
    "\n",
    "# Concatenate all the inputs\n",
    "input_layer = Concatenate(name='input_layer')([input_0d, embedding, flayer])\n",
    "\n",
    "# Add some 3 Dense layers\n",
    "nhid1, nhid2, nhid3 = 64, 32, 16 #size of the layers\n",
    "hidden1 = Dense(nhid1,activation = 'relu')(input_layer)\n",
    "hidden2 = Dense(nhid2,activation = 'relu')(hidden1)\n",
    "hidden3 = Dense(nhid3,activation = 'relu')(hidden2)\n",
    "output = Dense(1, activation = 'linear',name='output')(hidden3)\n",
    "\n",
    "## The final model\n",
    "nnmodel = Model([input_0d, input_cat, input_im],output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the model\n",
    "nnmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(nnmodel, 'my_nnmodel.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"my_nnmodel.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train into Val/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "np.random.seed(10)\n",
    "ids = shuffle(X.stormid.unique())\n",
    "\n",
    "#Take 80% for training\n",
    "limit_train = int(.8*len(ids))\n",
    "\n",
    "#Index of training/val\n",
    "idx_train = np.argwhere(X.stormid.isin(ids[:limit_train])).ravel()\n",
    "idx_val = np.argwhere(X.stormid.isin(ids[limit_train:])).ravel()\n",
    "\n",
    "Xin0D_train, Xcat_train, X2d_train, y_train = Xin0D.iloc[idx_train], Xcat.iloc[idx_train], X2d[idx_train],y.iloc[idx_train]\n",
    "Xin0D_val, Xcat_val, X2d_val, y_val = Xin0D.iloc[idx_val], Xcat.iloc[idx_val], X2d[idx_val],y.iloc[idx_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Standardization\n",
    "For all features (expect qualitative), transform your data such that mean=0 and std=1 (on the training data), and use the same parameters for transforming the test data also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_0D = StandardScaler().fit(Xin0D_train)\n",
    "scaler_im = lambda X: (X - X2d_train.mean(axis=(0,1,2),keepdims=True))/X2d_train.std(axis=(0,1,2),keepdims=True)\n",
    "\n",
    "Xin0D_train_scaled = scaler_0D.transform(Xin0D_train)\n",
    "Xin0D_val_scaled = scaler_0D.transform(Xin0D_val)\n",
    "Xin0D_test_scaled = scaler_0D.transform(Xin0D_test)\n",
    "\n",
    "\n",
    "X2d_train_scaled = scaler_im(X2d_train)\n",
    "X2d_val_scaled = scaler_im(X2d_val)\n",
    "X2d_test_scaled = scaler_im(X2d_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a optimizer and a loss\n",
    "nnmodel.compile(optimizer='Adam',loss='mean_squared_error')\n",
    "\n",
    "#Now train the model on very few epochs (to optimize try increasing the number of epochs)\n",
    "#Now train the model on very few epochs (to optimize try increasing the number of epochs)\n",
    "history  = nnmodel.fit([Xin0D_train_scaled, Xcat_train, X2d_train],y_train,\n",
    "            validation_data = ([Xin0D_val_scaled, Xcat_val, X2d_val], y_val),\n",
    "            epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot iterations\n",
    "plt.semilogy(history.history['loss'],color='gray',label='Training loss')\n",
    "plt.semilogy(history.history['val_loss'],color='black', label = 'Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean-Square Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = nnmodel.predict([Xin0D_val_scaled, Xcat_val, X2d_val])\n",
    "plt.scatter(y_val,y_val_predict)\n",
    "plt.plot([0,140],[0,140],'-k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "score = r2_score(y_val,y_val_predict)\n",
    "print('linear regression score: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predicting the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the name for the file (e.g. you last name, or a name for you algo)\n",
    "name = 'neural_net'\n",
    "\n",
    "y_test_predict = nnmodel.predict([Xin0D_test_scaled, Xcat_test, X2d_test])\n",
    "np.save('test_predict.' + name + '.npy',y_test_predict)\n",
    "\n",
    "#Send the file by email to julien.brajard@nersc.no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
